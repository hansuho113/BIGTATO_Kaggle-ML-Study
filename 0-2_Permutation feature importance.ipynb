{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutation Feature Importance\n",
    "\n",
    "- permutation Importance is calculated after a model has been fitted\n",
    "- we always compute permutation importance on test data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging이나 Boosting은 각각의 피처를 통한 시각화가 쉽지 않음.\n",
    "-> Feature importance 해석의 어려움이 있음 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn의 feature importance는 구현에 있어서 bias를 가짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CART : (Classification And Regression Tree)\n",
    "- 각 2개의 노드가 child 노드를 가지는 binary tree를 적절한 불순도(impurity)지표를 기준으로 생성해가는 알고리즘\n",
    "\n",
    "\n",
    "1. 분류 : 지니계수 및 엔트로피\n",
    "2. 회귀 : MSE(Mean Square Error)\n",
    "-> 이런 지표등을 이용해서 분산을 감소시키는 방향으로 노드 분할"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn에서는 지니중요도를 이용해서 각 feature의 중요도 측정\n",
    "- 해당 노드에서 샘플ㄷ르이 이질적으로 구성되어 있을수록 즉, 모든 class에 고루 분포되어 있을수록 지니불순도(impurity)는 높아지게 됨\n",
    "    - decesion tree는 이렇게 불순도를 감소시키는 방향으로 느드 생성 및 분류 진행\n",
    "    - 분류가 진행되며(노드가 분할되며) 노드들의 불순도는 작아짐(=순도(homogeneity) 증가)\n",
    "\n",
    "### * 노드 중요도 (Information gain)  : 발생할 확률이 적을수록 더 많은 정보량을 가짐\n",
    "#### I(x) = -log P(x)\n",
    "    information gain을 최대화하는 feature를 기준으로 노드 분할\n",
    "         - 어떤 노드의 노드 중요도 값이 클수록, 그 노드에서 특히 불순도가 크게 감소한다는 것을 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impurity-based feature importance : train set 에 학습되기 때문에 과적합될 가능성이 높으며 test set에 잘 적용되지 않을 수도 있음\n",
    "    - high cardinality features에 편향됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutation feature importance\n",
    "- 칼럼 데이터를 섞어서 연관성을 떨어뜨림(feature를 noise로 만드는 것임)\n",
    "    - > 이를 통해 얼마나 정확도에 영향을 미치는지 알 수 있음(교호작용도 포함)\n",
    "    - >  주의사항 : 무작위로 섞다보면 매우 비현실적인 데이터나 분석결과가 생길 수 있음. 따라서 상관관계가 매우 높을 경우는 고려해봐야함\n",
    "-  장점 : 재학습 시킬 필요가 없어서 학습 후에 적용하기 쉬우며 test set에 적용하므로 과적합 우려가 적음\n",
    "-  단, feature가 매우 많은 경우 적절히 선택해서 해야함(상관관계 정도에 따라)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cardinality data에 민감한 이유 : 노드를 쨀 게 많기 때문에"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference\n",
    "https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
